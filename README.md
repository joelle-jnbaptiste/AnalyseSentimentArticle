# Comparaison des approches de classification et d√©marche MLOps

_Auteur : JEAN BAPTISTE Jo√´lle_  
_Date : Avril 2025_

---

## Table des mati√®res

- [Introduction](#introduction)
- [Pr√©sentation et comparaison des mod√®les](#pr√©sentation-et-comparaison-des-mod√®les)
  - [Mod√®le sur mesure simple](#mod√®le-sur-mesure-simple)
  - [Mod√®le sur mesure avanc√©](#mod√®le-sur-mesure-avanc√©)
  - [Mod√®le avanc√© BERT](#mod√®le-avanc√©-bert)
- [D√©marche MLOps mise en ≈ìuvre](#d√©marche-mlops-mise-en-≈ìuvre)
  - [Principes g√©n√©raux](#principes-g√©n√©raux)
  - [Cycle MLOps du projet](#cycle-mlops-du-projet)
  - [Suivi des exp√©rimentations](#suivi-des-exp√©rimentations)
  - [Tests unitaires](#tests-unitaires)
  - [Architecture applicative d√©ploy√©e sur Azure](#architecture-applicative-d√©ploy√©e-sur-azure)
  - [D√©ploiement de l‚ÄôAPI](#d√©ploiement-de-lapi)
  - [Interface front de test (locale)](#interface-front-de-test-locale)
  - [Monitoring et alertes](#monitoring-et-alertes)
- [Proposition de d√©marche pour l'am√©lioration continue du mod√®le](#proposition-de-d√©marche-pour-lam√©lioration-continue-du-mod√®le)
- [Conclusion](#conclusion)

---

### üîó Liens vers les d√©p√¥ts du projet

- üíª **Backend (API FastAPI + mod√®le TFLite + d√©ploiement Azure)**  
  [https://github.com/joelle-jnbaptiste/Analyse_Sentiments](https://github.com/joelle-jnbaptiste/Analyse_Sentiments)

- üß™ **Frontend (Interface de test locale en Streamlit)**  
  [https://github.com/joelle-jnbaptiste/FrontAnalyseSentiment](https://github.com/joelle-jnbaptiste/FrontAnalyseSentiment)

---

## Introduction

Dans le cadre de ce projet, nous avons con√ßu et compar√© plusieurs approches de classification de sentiments afin d'√©valuer leur pertinence en situation r√©elle.  
L‚Äôobjectif √©tait double : √©valuer les performances de diff√©rents types de mod√®les ‚Äî du plus simple au plus avanc√© ‚Äî et mettre en place une d√©marche MLOps compl√®te garantissant un suivi, une reproductibilit√© et une am√©lioration continue du mod√®le en production.

Trois approches ont √©t√© √©tudi√©es :

- Un mod√®le sur mesure simple, bas√© sur des techniques classiques de vectorisation et de classification.
- Un mod√®le sur mesure avanc√©, int√©grant des m√©thodes d‚Äôapprentissage plus complexes.
- Un mod√®le avanc√© bas√© sur BERT, une architecture de type Transformer pr√©-entra√Æn√©e, convertie ici au format TFLite pour faciliter son d√©ploiement.

Pour accompagner ces exp√©rimentations, une architecture orient√©e MLOps a √©t√© mise en place, int√©grant le tracking des mod√®les via MLflow, le d√©ploiement via Docker et Azure Container Apps, ainsi que le monitoring et les alertes via Azure Application Insights.

Ce document pr√©sente la d√©marche, les r√©sultats de comparaison des mod√®les et la mani√®re dont les principes MLOps ont √©t√© appliqu√©s √† ce projet.

---

## Pr√©sentation et comparaison des mod√®les

### Objectif de la mod√©lisation

L‚Äôobjectif de cette √©tape est d‚Äôidentifier le mod√®le d‚Äôapprentissage supervis√© le plus performant pour la t√¢che de classification binaire √† partir de donn√©es textuelles. Plusieurs familles de mod√®les ont √©t√© test√©es afin d‚Äô√©valuer diff√©rentes approches de repr√©sentation des donn√©es (vectorisation) et de mod√©lisation (mod√®les classiques, r√©seaux de neurones profonds, mod√®les pr√©entra√Æn√©s).

Pour cela, nous avons entra√Æn√© :

- des mod√®les **classiques**, comme la **r√©gression logistique** utilisant une vectorisation **TF-IDF** ;
- des mod√®les **de deep learning**, tels que des r√©seaux de neurones **LSTM**, **CNN** et des architectures **hybrides CNN-LSTM**, combin√©s √† des embeddings entra√Æn√©s de type **Word2Vec** ou **FastText** ;
- un mod√®le de **type transformer**, en l‚Äôoccurrence **BERT**, pr√©entra√Æn√© sur un large corpus puis fine-tun√© sur notre jeu de donn√©es, afin de tester l‚Äôapport d‚Äôune compr√©hension contextuelle avanc√©e du texte.

L‚Äô√©valuation porte non seulement sur la performance globale (pr√©cision, rappel, f1-score), mais √©galement sur la capacit√© des mod√®les √† g√©n√©raliser, la stabilit√© des pr√©dictions et le taux d‚Äôerreur sur les classes minoritaires. Cela permet de faire un choix √©clair√© en tenant compte des compromis entre complexit√©, efficacit√© et robustesse du mod√®le.

### Mod√®les explor√©s

Afin de comparer diff√©rentes approches de classification sur des donn√©es textuelles, plusieurs types de mod√®les ont √©t√© s√©lectionn√©s. Le choix s‚Äôest port√© sur des architectures vari√©es, allant des plus simples aux plus sophistiqu√©es, afin d‚Äô√©valuer √† la fois la performance, la capacit√© de g√©n√©ralisation et la complexit√© de mise en ≈ìuvre.

#### Mod√®les classiques

- **R√©gression logistique** : utilis√©e comme **mod√®le de r√©f√©rence**, elle a √©t√© entra√Æn√©e sur des donn√©es vectoris√©es avec l‚Äôapproche **TF-IDF**. Ce mod√®le simple permet d‚Äô√©tablir une baseline solide, rapide √† entra√Æner et facile √† interpr√©ter.

#### R√©seaux de neurones s√©quentiels

- **LSTM** : les mod√®les √† m√©moire courte longue dur√©e sont utilis√©s pour capturer les d√©pendances dans les s√©quences de mots. Plusieurs variantes ont √©t√© test√©es, avec des embeddings **Word2Vec** et **FastText**, associ√©s √† diff√©rents pr√©traitements (lemmatisation, suppression des stopwords...).

#### R√©seaux convolutionnels

- **CNN** : initialement d√©velopp√©s pour le traitement d‚Äôimages, les r√©seaux convolutifs ont √©galement d√©montr√© de bonnes performances sur les textes, notamment pour d√©tecter des motifs locaux. Les mod√®les CNN ont √©t√© √©valu√©s avec diff√©rents types d‚Äôembeddings.

#### Architectures hybrides

- **CNN-LSTM** : ce mod√®le combine les avantages des convolutions (d√©tection locale de motifs) et des LSTM (compr√©hension du contexte global) pour am√©liorer la capture de la structure des s√©quences textuelles.

#### Mod√®le pr√©entra√Æn√© de type transformer

- **BERT** : nous avons fine-tun√© un mod√®le **BERT (Bidirectional Encoder Representations from Transformers)** sur notre jeu de donn√©es. Ce type de mod√®le est capable de capturer des relations contextuelles riches entre les mots dans un texte, et constitue aujourd‚Äôhui l‚Äô√©tat de l‚Äôart en NLP.

Chaque mod√®le a √©t√© test√© avec des variantes de pr√©traitement pour √©valuer l‚Äôimpact de la lemmatisation, de la suppression des stopwords, ou encore du type d‚Äôembedding utilis√©.

### Pr√©traitement appliqu√©

Avant l'entra√Ænement des mod√®les, un travail de nettoyage et de transformation du texte a √©t√© effectu√© afin d‚Äôam√©liorer la qualit√© des repr√©sentations et la performance des mod√®les. Plusieurs configurations ont √©t√© test√©es afin d‚Äô√©valuer leur impact.

#### Nettoyage du texte

Les op√©rations suivantes ont √©t√© appliqu√©es de mani√®re syst√©matique :

- Conversion en minuscules
- Suppression de la ponctuation et des caract√®res sp√©ciaux
- Suppression des URLs et mentions (si pr√©sentes)

#### Lemmatisation et suppression des stopwords

Deux versions principales du corpus ont √©t√© utilis√©es :

- Une version **lemmatis√©e**, o√π chaque mot est r√©duit √† sa forme canonique (ex. "mangeaient" ‚Üí "manger") ;
- Une version avec ou sans **stopwords** (mots fr√©quents comme "le", "et", "est", souvent peu informatifs).

#### Repr√©sentation vectorielle

Plusieurs techniques d‚Äôencodage du texte ont √©t√© explor√©es :

- **TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)** : utilis√©e avec la r√©gression logistique, cette m√©thode permet de transformer chaque texte en vecteur num√©rique bas√© sur la fr√©quence des mots pond√©r√©e par leur raret√©.

- **Word embeddings statiques** :
  - **Word2Vec** : capture des relations s√©mantiques entre les mots √† partir de leurs contextes locaux.
  - **FastText** : am√©liore Word2Vec en prenant en compte les sous-mots, ce qui permet une meilleure gestion des mots rares ou mal orthographi√©s.

Ces embeddings ont √©t√© soit entra√Æn√©s en amont sur un corpus externe, soit import√©s depuis des mod√®les pr√©entra√Æn√©s disponibles publiquement.

- **BERT tokenizer** : pour le mod√®le BERT, les textes ont √©t√© encod√©s via le tokenizer officiel, qui d√©coupe les phrases en **WordPieces** et ajoute les **tokens sp√©ciaux** ([CLS], [SEP], etc.) requis pour le traitement par le mod√®le transformer.

L‚Äôensemble des mod√®les a ainsi √©t√© test√© avec diff√©rentes combinaisons : avec ou sans lemmatisation, avec ou sans stopwords, et selon le type d‚Äôembedding utilis√©. Ces variations permettent d‚Äôanalyser leur influence sur la performance finale.

### M√©triques d‚Äô√©valuation

Afin de comparer efficacement les performances des mod√®les de classification, plusieurs m√©triques ont √©t√© s√©lectionn√©es. Elles permettent d‚Äô√©valuer non seulement la pr√©cision globale, mais aussi la qualit√© des pr√©dictions pour chacune des classes, en particulier dans un contexte de d√©s√©quilibre √©ventuel.

#### Accuracy

L‚Äô**accuracy** mesure la proportion globale de bonnes pr√©dictions. Bien qu‚Äôintuitive, cette m√©trique peut √™tre trompeuse lorsque les classes sont d√©s√©quilibr√©es.

#### F1-score

Le **f1-score** est la moyenne harmonique entre la pr√©cision (precision) et le rappel (recall). C‚Äôest une m√©trique particuli√®rement adapt√©e lorsque l‚Äôon cherche un compromis entre les **faux positifs** et les **faux n√©gatifs**. Elle a √©t√© utilis√©e comme **m√©trique principale de comparaison** entre les mod√®les.

#### Recall (sensibilit√©)

Le **recall** indique la capacit√© du mod√®le √† d√©tecter les exemples positifs. Il est crucial lorsque l‚Äôobjectif est de **limiter les faux n√©gatifs**, par exemple pour ne pas rater des cas importants √† d√©tecter.

#### Precision

La **precision** mesure la proportion de vrais positifs parmi toutes les pr√©dictions positives. Elle est importante lorsque l‚Äôon cherche √† **limiter les faux positifs**.

#### ROC AUC

Le **ROC AUC (Receiver Operating Characteristic - Area Under Curve)** √©value la capacit√© du mod√®le √† s√©parer les classes, ind√©pendamment du seuil de d√©cision. Une valeur proche de 1 indique une excellente capacit√© de s√©paration.

#### Log Loss

La **log loss** (ou perte logarithmique) prend en compte la **probabilit√©** associ√©e √† chaque pr√©diction. Plus les pr√©dictions sont proches de la r√©alit√© avec des scores de confiance √©lev√©s, plus la log loss est faible. Cette m√©trique est utile pour √©valuer la **calibration** du mod√®le.

#### Autres m√©triques suivies

- **Standard deviation de confiance (`confidence_std`)** : permet d‚Äô√©valuer la **stabilit√© des pr√©dictions** entre les exemples.
- **Nombre de faux positifs / faux n√©gatifs** : pour une interpr√©tation plus directe des erreurs.

Ces diff√©rentes m√©triques ont √©t√© enregistr√©es pour chaque mod√®le test√©, via **MLflow**, afin d‚Äôassurer une tra√ßabilit√© et une comparaison fine des performances.

### Comparaison des r√©sultats

#### Dummy Classifier ‚Äì Mod√®le de r√©f√©rence na√Øf

Le **Dummy Classifier** a √©t√© utilis√© comme **mod√®le de r√©f√©rence minimal**. Il ne repose sur aucune analyse du contenu textuel, mais pr√©dit syst√©matiquement la m√™me classe, ici la classe majoritaire. Ce type de mod√®le permet d‚Äô√©tablir un point de comparaison de base, afin de s‚Äôassurer que les mod√®les plus complexes apportent une r√©elle valeur ajout√©e.

Sans surprise, ses performances sont √©quivalentes au hasard :

- `f1_score = 0.50`
- `accuracy = 0.50`
- `recall = 0.50`
- `precision = 0.50`
- `roc_auc = 0.50`

La matrice de confusion montre que le mod√®le **pr√©dit uniquement la classe positive**, ignorant totalement la classe n√©gative. Cela entra√Æne **751 faux positifs** et **750 vrais positifs**, mais **aucune capacit√© de discrimination**.

De plus, la courbe ROC confirme ce constat, avec une AUC de 0.50, ce qui signifie que le mod√®le ne fait **aucune distinction** entre les classes.

_Illustration ‚Äì Matrice de confusion du Dummy Classifier :_
![Matrice de confusion ‚Äì Dummy Classifier](images/dummy2.png)

_Illustration ‚Äì Courbe ROC du Dummy Classifier :_  
![Courbe ROC ‚Äì Dummy Classifier](images/dummy3.png)

Ce mod√®le, bien que totalement inadapt√© √† une t√¢che r√©elle de classification, est un **point de d√©part essentiel** pour √©valuer les am√©liorations apport√©es par les mod√®les plus avanc√©s.

#### TF-IDF + R√©gression Logistique

Le mod√®le bas√© sur une **r√©gression logistique** utilisant une vectorisation **TF-IDF** constitue une approche classique et robuste pour la classification de texte. Il agit comme un bon compromis entre performance, rapidit√© d'entra√Ænement et interpr√©tabilit√©.

Les r√©sultats obtenus sont nettement sup√©rieurs √† ceux du mod√®le de r√©f√©rence (Dummy) :

- `f1_score = 0.73`
- `accuracy = 0.72`
- `precision = 0.70`
- `recall = 0.75`
- `roc_auc = 0.79`
- `log_loss = 0.56`
- `confidence_std = 0.21`
- `faux n√©gatifs = 186`
- `faux positifs = 237`

La **matrice de confusion** montre que le mod√®le est capable de correctement identifier une grande majorit√© d'exemples positifs et n√©gatifs. On observe un **√©quilibre raisonnable entre rappel et pr√©cision**, m√™me si le nombre de faux positifs reste cons√©quent.

_Illustration ‚Äì Matrice de confusion du mod√®le TF-IDF + LogReg :_  
![Confusion Matrix ‚Äì TF-IDF + LogReg](images/tfidf1.png)

_Illustration ‚Äì Courbe ROC :_  
![ROC Curve ‚Äì TF-IDF + LogReg](images/tfid2.png)

Ce mod√®le d√©montre qu'une approche simple, bien param√©tr√©e, peut d√©j√† fournir de tr√®s bons r√©sultats. Il constitue un excellent **baseline "fort"** √† comparer aux mod√®les de deep learning plus co√ªteux √† entra√Æner.

#### LSTM

Le mod√®le **LSTM** (Long Short-Term Memory) est une architecture de r√©seau de neurones r√©current bien adapt√©e aux s√©quences textuelles. Dans cette version, nous avons utilis√© des embeddings **Word2Vec** sur des textes **lemmatis√©s sans stopwords**, afin de simplifier la repr√©sentation s√©mantique tout en conservant le sens des mots importants.

Les performances observ√©es restent **modestes** malgr√© une architecture plus complexe :

- `f1_score = 0.61`
- `accuracy = 0.54`
- `recall = 0.71`
- `precision = 0.53`
- `roc_auc = 0.58`
- `log_loss = 0.68`
- `confidence_std = 0.05`
- `faux n√©gatifs = 214`
- `faux positifs = 474`

L‚Äô**am√©lioration du rappel** montre que le mod√®le est plus apte √† d√©tecter les exemples positifs, mais au prix d‚Äôun **nombre √©lev√© de faux positifs**. Cela refl√®te un certain d√©s√©quilibre dans la prise de d√©cision, peut-√™tre d√ª √† une g√©n√©ralisation insuffisante ou √† un surapprentissage sur certains motifs peu discriminants.

_Illustration ‚Äì Matrice de confusion :_  
![Confusion Matrix ‚Äì LSTM Word2Vec](images/lstm1.png)

_Illustration ‚Äì Courbe ROC :_  
![ROC Curve ‚Äì LSTM Word2Vec](images/lstm2.png)

_Illustration ‚Äì R√©capitulatif complet dans MLflow :_  
![MLflow Run Overview ‚Äì LSTM](images/lstmBest.png)

_Illustration ‚Äì Comparaison des mod√®les dans MLflow (1/2) :_  
![Comparaison LSTM ‚Äì Partie 1](images/lstmComparison1.png)

_Illustration ‚Äì Comparaison des mod√®les dans MLflow (2/2) :_  
![Comparaison LSTM ‚Äì Partie 2](images/lstmComparison2.png)

Bien que ce LSTM capte partiellement les dynamiques temporelles du texte, il **reste limit√©** en performance par rapport aux mod√®les classiques comme la r√©gression logistique, et encore plus par rapport aux architectures transformers comme BERT.

#### CNN

Le mod√®le **CNN** (Convolutional Neural Network) est une architecture initialement con√ßue pour les images, mais qui s‚Äôest r√©v√©l√©e efficace pour les textes en capturant des **motifs locaux** √† travers des filtres convolutifs. Dans cette configuration, il s‚Äôappuie sur des **embeddings Word2Vec** extraits de textes **lemmatis√©s sans stopwords**, permettant une repr√©sentation compacte et significative des s√©quences.

Les performances du mod√®le CNN se r√©v√®lent **solides**, surpassant les LSTM sur plusieurs points :

- `f1_score = 0.63`
- `accuracy = 0.59`
- `recall = 0.68`
- `precision = 0.58`
- `roc_auc = 0.64`
- `log_loss = 0.74`
- `confidence_std = 0.25`
- `faux n√©gatifs = 240`
- `faux positifs = 369`

Avec un **rappel √©lev√©** et une **pr√©cision √©quilibr√©e**, le mod√®le d√©montre une bonne capacit√© √† identifier les exemples positifs, tout en conservant une mod√©ration sur les fausses alertes. Le score AUC √† 0.64, bien au-dessus du hasard, confirme sa capacit√© √† distinguer les classes. Toutefois, la log loss relativement √©lev√©e indique une certaine incertitude dans ses pr√©dictions.

_Illustration ‚Äì Matrice de confusion :_  
![Confusion Matrix ‚Äì CNN Word2Vec](images/cnn1.png)

_Illustration ‚Äì Courbe ROC :_  
![ROC Curve ‚Äì CNN Word2Vec](images/cnn2.png)

_Illustration ‚Äì R√©capitulatif complet dans MLflow :_  
![MLflow Run Overview ‚Äì CNN Word2Vec](images/cnnBest.png)

_Illustration ‚Äì Comparaison des mod√®les dans MLflow (1/2) :_  
![Comparaison CNN ‚Äì Partie 1](images/cnnComparison1.png)

_Illustration ‚Äì Comparaison des mod√®les dans MLflow (2/2) :_  
![Comparaison CNN ‚Äì Partie 2](images/cnnComparison2.png)

Gr√¢ce √† sa structure simple mais efficace, le **CNN Word2Vec** repr√©sente une alternative int√©ressante, surtout dans les contextes o√π l‚Äôon cherche un bon √©quilibre entre performance et rapidit√© d'entra√Ænement.

#### CNN + LSTM + FastText (racine sans stopwords)

Dans cette derni√®re configuration, nous avons fusionn√© deux approches compl√©mentaires : les **r√©seaux convolutifs (CNN)**, capables de d√©tecter des motifs locaux (comme des expressions cl√©s), et les **r√©seaux LSTM**, plus adapt√©s aux d√©pendances longues dans le texte. Les textes ont √©t√© transform√©s via des embeddings **FastText**, apr√®s un **stemming** et suppression des **stopwords**, afin d‚Äôobtenir une repr√©sentation dense et g√©n√©ralisable.

Cependant, malgr√© la sophistication de l‚Äôarchitecture, les performances globales restent **d√©cevantes**, proches d‚Äôun comportement al√©atoire :

- `f1_score = 0.58`
- `accuracy = 0.49`
- `recall = 0.70`
- `precision = 0.50`
- `roc_auc = 0.52`
- `log_loss = 0.69`
- `confidence_std = 0.01`
- `faux n√©gatifs = 223`
- `faux positifs = 532`

On observe un **d√©s√©quilibre marqu√©** dans les pr√©dictions positives, avec un nombre √©lev√© de faux positifs. La courbe ROC quasiment diagonale confirme l‚Äô**absence de pouvoir discriminant** du mod√®le. Cette contre-performance peut r√©sulter d‚Äôun **apprentissage peu convergent** d√ª √† la complexit√© du mod√®le, √† un surajustement rapide ou √† une combinaison peu harmonieuse entre les deux architectures.

_Illustration ‚Äì Matrice de confusion :_  
![Confusion Matrix ‚Äì CNN + LSTM FastText](images/cnnLstm1.png)

_Illustration ‚Äì Courbe ROC :_  
![ROC Curve ‚Äì CNN + LSTM FastText](images/cnnLstm2.png)

_Illustration ‚Äì R√©capitulatif complet dans MLflow :_  
![MLflow Run Overview ‚Äì CNN + LSTM](images/cnnLstmBest.png)

_Illustration ‚Äì Comparaison des mod√®les dans MLflow (1/2) :_  
![Comparaison CNN+LSTM ‚Äì Partie 1](images/cnnLstmComparison1.png)

_Illustration ‚Äì Comparaison des mod√®les dans MLflow (2/2) :_  
![Comparaison CNN+LSTM ‚Äì Partie 2](images/cnnLstmComparison2.png)

Cette exp√©rience montre que la **complexit√© d‚Äôun mod√®le ne garantit pas n√©cessairement de meilleures performances**, surtout si la phase de r√©glage (hyperparam√®tres, r√©gularisation, taille des donn√©es) n‚Äôest pas suffisamment pouss√©e.

#### DistilBERT

Le mod√®le **DistilBERT** est une version all√©g√©e du mod√®le BERT, pr√©-entra√Æn√©e pour capturer les repr√©sentations s√©mantiques profondes du langage naturel. Il b√©n√©ficie d‚Äôun apprentissage supervis√© par distillation √† partir de BERT, ce qui lui permet de conserver de tr√®s bonnes performances tout en √©tant plus l√©ger et plus rapide √† entra√Æner.

Ce mod√®le se d√©marque nettement des pr√©c√©dents en offrant **les meilleures performances** de tous les mod√®les test√©s :

- `f1_score = 0.78`
- `accuracy = 0.79`
- `recall = 0.76`
- `precision = 0.81`
- `roc_auc = 0.87`
- `log_loss = 0.68`
- `confidence_std = 0.45`
- `faux n√©gatifs = 183`
- `faux positifs = 137`

Il pr√©sente un excellent √©quilibre entre **pr√©cision** et **rappel**, tout en affichant un score AUC √©lev√©, ce qui montre une capacit√© robuste √† distinguer les classes. En revanche, la variance de confiance (`confidence_std`) plus √©lev√©e pourrait indiquer une plus grande sensibilit√© aux exemples ambigus ou atypiques du jeu de test.

_Illustration ‚Äì Matrice de confusion :_  
![Confusion Matrix ‚Äì DistilBERT](images/bert1.png)

_Illustration ‚Äì Courbe ROC :_  
![ROC Curve ‚Äì DistilBERT](images/bert2.png)

_Illustration ‚Äì R√©capitulatif complet dans MLflow :_  
![MLflow Run Overview ‚Äì DistilBERT](images/bestModel.png)

Ce mod√®le confirme la **sup√©riorit√© des transformers** pour la classification de texte, m√™me avec un entra√Ænement limit√© √† 3 √©poques, et justifie leur adoption dans les applications industrielles de NLP.

#### Synth√®se des performances et limites des mod√®les

L‚Äôensemble des mod√®les test√©s ‚Äì allant de la **r√©gression logistique TF-IDF**, aux r√©seaux **LSTM**, **CNN**, jusqu‚Äô√† la fusion **CNN + LSTM**, puis **DistilBERT** ‚Äì permet de mettre en lumi√®re une √©volution progressive des performances‚Ä¶ mais aussi leurs limites respectives.

| Mod√®le                | F1-Score | Accuracy | AUC      |
| --------------------- | -------- | -------- | -------- |
| Dummy Classifier      | 0.50     | 0.50     | 0.50     |
| TF-IDF + LogReg       | 0.73     | 0.72     | 0.79     |
| LSTM + Word2Vec       | 0.61     | 0.54     | 0.58     |
| CNN + Word2Vec        | 0.63     | 0.59     | 0.64     |
| CNN + LSTM + FastText | 0.58     | 0.50     | 0.52     |
| **DistilBERT**        | **0.78** | **0.79** | **0.87** |

L‚Äôapproche **TF-IDF + LogReg** surpasse tous les mod√®les de type s√©quentiel classiques (LSTM, CNN) en pr√©cision et en stabilit√©, probablement gr√¢ce √† sa simplicit√© et √† sa meilleure capacit√© √† g√©n√©raliser avec des donn√©es limit√©es. Les architectures profondes comme LSTM et CNN, bien que th√©oriquement adapt√©es au texte, peinent √† extraire des signaux robustes ici, souvent frein√©es par le manque de volume ou de qualit√© s√©mantique du dataset.  

L‚Äô**arriv√©e de DistilBERT** change la donne :  

- `f1_score = 0.78`  
- `accuracy = 0.79`  
- `roc_auc = 0.87`  
- `precision = 0.81`  
- `recall = 0.76`  
- `log_loss = 0.67`  
- `false_positives = 137`  
- `false_negatives = 183`

Gr√¢ce √† sa compr√©hension contextuelle plus fine, **DistilBERT** capte les nuances du langage avec une **sup√©riorit√© nette** sur les autres mod√®les. La r√©duction drastique des erreurs, notamment des faux positifs, t√©moigne de son efficacit√©.

 _Illustration ‚Äì Comparatif global (1/2) :_  
![Comparaison globale ‚Äì Partie 1](images/overallComparison.png)

 _Illustration ‚Äì Comparatif global (2/2) :_  
![Comparaison globale ‚Äì Partie 2](images/overallComparison2.png)

---

### Ouverture ‚Äì Les erreurs de DistilBERT : un d√©fi s√©mantique

Malgr√© sa performance impressionnante, DistilBERT n‚Äôest pas infaillible. L‚Äôanalyse des **erreurs enregistr√©es dans MLflow** met en √©vidence un type d‚Äôambigu√Øt√© r√©current :  

> Des phrases contenant du **sarcasme**, de la **d√©rision** ou des tournures humoristiques √©chappent au mod√®le.

 _Exemples d‚Äôerreurs typiques_ :

- _"New fave..bad girlfriend theory of a dead man..ck it out"_
- _"Dying in lines with tamttity, joe, and dylan. MTVMA party "_
- _"Thankfully there‚Äôs Stanley Cup hockey on to help ease my bummed out mood."_

Ces exemples illustrent le **manque de discernement sur le ton** du message : un probl√®me bien connu des mod√®les NLP, m√™me avanc√©s. Le texte semble positif en surface, mais le **sous-texte √©motionnel** est en r√©alit√© **n√©gatif**.

_Illustration ‚Äì Analyse des erreurs de BERT :_  
![Erreurs de pr√©diction ‚Äì DistilBERT](images/bertErrors.png)

---

 Une **piste d‚Äôam√©lioration** serait d‚Äôenrichir le jeu de donn√©es avec des labels contextuels (ex. : sarcasme, ironie, √©motion dominante) ou de coupler BERT √† un d√©tecteur de tonalit√© ou d‚Äôintention, ouvrant vers des **mod√®les multimodaux** ou des **architectures sp√©cialis√©es en pragmatique du langage**.

---

## D√©marche MLOps mise en ≈ìuvre

### Principes g√©n√©raux

Dans ce projet, l‚Äôapproche MLOps permet de structurer le cycle de vie du mod√®le de classification de sentiment, de l‚Äôexp√©rimentation jusqu‚Äôau suivi en production.

Les objectifs sont les suivants :

- **Reproductibilit√©** : pouvoir relancer les entra√Ænements et obtenir les m√™mes r√©sultats
- **Tra√ßabilit√©** : conserver l‚Äôhistorique des essais, des m√©triques et des versions de mod√®les
- **Fiabilit√©** : tester automatiquement les composants critiques
- **Automatisation** : d√©ploiement reproductible via Docker, h√©bergement cloud
- **Observabilit√©** : supervision en production, alertes sur d√©rives ou erreurs
- **Am√©lioration continue** : collecte de feedbacks utilisateurs pour affiner le mod√®le

Ces principes ont guid√© la mise en ≈ìuvre technique d√©crite dans les sous-sections suivantes.

### Cycle MLOps du projet

Le sch√©ma suivant illustre la **vision globale du cycle MLOps** mise en place dans le projet.  
On y retrouve les phases principales allant du d√©veloppement √† la mise en production, en passant par la validation, la surveillance et une proposition de boucle d'am√©lioration continue.

![Cycle MLOps](images/cycleMLOps.png)

### Suivi des exp√©rimentations

Toutes les exp√©rimentations ont √©t√© suivies √† l‚Äôaide de **MLflow**, un outil open source de gestion du cycle de vie des mod√®les de machine learning.

Nous avons utilis√© **MLflow Tracking** pour enregistrer :

- Les **hyperparam√®tres** (par exemple `max_features`, `model_type`, etc.)
- Les **scores de performance** (accuracy, F1-score, etc.)
- Les **art√©facts** tels que les mod√®les entra√Æn√©s, les tokenizers, et les logs
- Les **tags** permettant de regrouper ou filtrer des essais par type de mod√®le ou objectif

Chaque ex√©cution (`run`) est associ√©e √† un identifiant unique, une date, et une version du code, ce qui permet de revenir en arri√®re ou de reproduire un r√©sultat √† tout moment.

L‚Äôinterface web de MLflow a √©galement permis de comparer visuellement les r√©sultats des diff√©rents mod√®les :

- Comparaison de courbes de performances
- Tri des runs par m√©triques
- R√©cup√©ration directe des mod√®les les plus performants

Cette tra√ßabilit√© a √©t√© essentielle pour identifier la meilleure approche √† d√©ployer en production.

### Versionnement et stockage des mod√®les

Dans ce projet, le **versionnement du code source et du mod√®le** a √©t√© assur√© uniquement via **Git**, ce qui s'est av√©r√© suffisant gr√¢ce √† la l√©g√®ret√© du mod√®le final utilis√© (format `.tflite`).

Contrairement √† une architecture MLOps compl√®te avec stockage cloud (Blob Storage, S3, etc.), nous avons choisi une approche plus simple mais efficace : le mod√®le final est int√©gr√© directement dans le d√©p√¥t du projet, aux c√¥t√©s du tokenizer.

Ce choix est rendu possible gr√¢ce √† l‚Äôutilisation de **TensorFlow Lite**, qui g√©n√®re un mod√®le compress√© facile √† embarquer dans une API ou un conteneur Docker, sans compromettre les performances.

Les exp√©rimentations ont quant √† elles √©t√© suivies via **MLflow**, mais **le stockage et le d√©ploiement du mod√®le retenu** reposent uniquement sur le versionnement Git.  
Cela garantit une bonne tra√ßabilit√© tout en limitant la complexit√© d‚Äôinfrastructure.

**R√©sum√©** :

- ‚úÖ Code versionn√© avec Git
- ‚úÖ Mod√®le `.tflite` lightweight versionn√© dans le d√©p√¥t
- ‚úÖ MLflow utilis√© pour la comparaison des exp√©rimentations, pas pour le d√©ploiement

---

### Tests unitaires

Des tests unitaires ont √©t√© mis en place afin de garantir la fiabilit√© du mod√®le de classification avant sa mise en production. Ces tests ont √©t√© con√ßus pour valider les points critiques du pipeline d'inf√©rence. Ils sont regroup√©s dans un r√©pertoire `tests/` et ex√©cut√©s avec **pytest**.

Trois tests principaux ont √©t√© d√©finis :

1. **Test de pr√©diction positive**  
   Ce test v√©rifie que le mod√®le est capable d‚Äôidentifier correctement un texte clairement positif.  
   Il confirme que la pr√©diction retourn√©e est bien `1` et que la sortie respecte le bon format (`list` contenant un `int`).

2. **Test de pr√©diction n√©gative**  
   Similaire au test pr√©c√©dent, il v√©rifie que le mod√®le retourne `0` pour une phrase n√©gative.  
   Il permet de s‚Äôassurer du comportement attendu sur les cas simples.

3. **Test de robustesse sur un lot de phrases**  
   Le mod√®le est test√© sur plusieurs exemples positifs et n√©gatifs, avec une v√©rification du taux de bonnes pr√©dictions.  
   Ce test permet de valider la stabilit√© du mod√®le sur des entr√©es vari√©es.

Ces tests sont automatiquement ex√©cut√©s √† chaque **pull request** via une **action GitHub**. Cela permet de valider les modifications avant int√©gration.

Bien que la version gratuite de GitHub ne permette pas de bloquer une fusion en cas d‚Äô√©chec, cette pratique est essentielle en contexte professionnel, o√π les workflows peuvent √™tre configur√©s comme **bloquants** (CI obligatoire avant merge).

Voici deux captures d'√©cran illustrant cette int√©gration :

#### Aper√ßu des ex√©cutions de workflow GitHub Actions

![Aper√ßu GitHub Actions](images/githubAction.png)

#### Exemple de rapport de tests ex√©cut√©s avec `pytest`

![Rapport pytest GitHub](images/githubTest.png)

Cette strat√©gie permet de s√©curiser la phase de d√©veloppement, de d√©tecter rapidement les erreurs, et de favoriser une meilleure qualit√© de code dans le temps.

---

### Architecture applicative d√©ploy√©e sur Azure

Le diagramme suivant pr√©sente les diff√©rents composants du projet et leur interaction entre la **partie locale (frontend Streamlit)** et l‚Äô**infrastructure cloud (API d√©ploy√©e, Application Insights, alerting)**.

On y visualise :

- Le fonctionnement du front local en interaction avec l‚ÄôAPI (`/predict` et `/feedback`)
- Le traitement de ces requ√™tes via le container Azure
- La journalisation des feedbacks dans Application Insights
- Le syst√®me d‚Äôalerte d√©clench√© en cas d‚Äôerreurs r√©p√©t√©es

![Architecture Azure](images/application.png)

---

### D√©ploiement de l‚ÄôAPI

L‚ÄôAPI a √©t√© d√©velopp√©e avec **FastAPI**, un framework l√©ger et rapide pour cr√©er des services web en Python. Elle expose deux endpoints : `/predict` pour la pr√©diction de sentiment, et `/feedback` pour la collecte de retour utilisateur.  
Une fois test√©e en local, elle a √©t√© conteneuris√©e puis d√©ploy√©e sur Azure.

#### Conteneurisation avec Docker

L‚Äôensemble du projet (API, mod√®le TFLite, tokenizer) est empaquet√© dans une image Docker.  
Cela garantit une ex√©cution reproductible en local comme en cloud. Les d√©pendances sont d√©finies dans un fichier `requirements-api.txt`.

Avant de proc√©der au d√©ploiement sur Azure, il est possible de tester l‚ÄôAPI **en local** √† l‚Äôaide des commandes suivantes :

```bash
# Construction de l‚Äôimage Docker
docker build -t sentiment-api .

# Lancement du conteneur localement
docker run -p 8000:8000 sentiment-api
```

Une fois le conteneur lanc√©, l‚ÄôAPI est accessible √† l‚Äôadresse suivante dans le navigateur :

```
http://localhost:8000/docs
```

Cela permet de tester les endpoints `/predict` et `/feedback` via Swagger, comme si elle √©tait d√©j√† en ligne.

L‚Äôimage est ensuite pouss√©e dans un **container registry Azure** et d√©ploy√©e automatiquement via GitHub Actions.

![Pipeline de d√©ploiement GitHub](images/pipelineDeploiment.png)

---

#### D√©ploiement sur Azure Container Apps

L‚Äôimage est d√©ploy√©e dans **Azure Container Apps**, un service manag√© qui permet d‚Äôex√©cuter l‚ÄôAPI sans g√©rer de VM.  
Cette solution assure la scalabilit√© automatique, un monitoring natif et une int√©gration facile avec Application Insights.

![Instance Azure Container App](images/containerApp.png)

#### Endpoints disponibles et testables

Une fois l‚ÄôAPI d√©ploy√©e, elle est accessible publiquement et peut √™tre test√©e via l‚Äôinterface Swagger g√©n√©r√©e automatiquement par FastAPI.

Voici les endpoints disponibles :

- `/predict` : re√ßoit une cha√Æne de caract√®res et retourne une pr√©diction (`0` ou `1`)
- `/feedback` : enregistre le feedback utilisateur sur une pr√©diction

![Swagger UI FastAPI](images/swagger.png)

#### Exemple d‚Äôappel √† `/predict`

```json
{
  "text": "string"
}
```

![Test de l‚Äôendpoint predict](images/endpointPredict.png)

#### Exemple d‚Äôappel √† `/feedback`

```json
{
  "texte": "string",
  "prediction": 0,
  "feedback_correct": true
}
```

![Test de l‚Äôendpoint feedback](images/endpointFeedback.png)

Ce d√©ploiement permet de rendre le mod√®le disponible √† des utilisateurs externes, tout en facilitant la supervision et les mises √† jour gr√¢ce √† la conteneurisation et √† l‚Äôautomatisation des workflows.

---

### Interface front de test (locale)

Pour permettre une √©valuation simple de l‚ÄôAPI sans passer par des outils externes, une interface utilisateur l√©g√®re a √©t√© d√©velopp√©e avec **Streamlit**.

Ce front-end n‚Äôest pas d√©ploy√© sur le cloud, mais peut √™tre lanc√© localement pour :

- Tester le comportement du mod√®le (`/predict`)
- Visualiser la pr√©diction sous forme lisible (positif/n√©gatif)
- Soumettre un retour utilisateur (`/feedback`)
- Envoyer les feedbacks en un clic

#### Fonctionnement

L‚Äôutilisateur entre un message libre dans un champ texte, clique sur "Analyser le sentiment", et obtient une r√©ponse imm√©diate.  
Il peut ensuite valider ou invalider la pr√©diction via une interface radio + bouton, ce qui d√©clenche l‚Äôappel au second endpoint.

Voici quelques captures d‚Äô√©cran illustrant le parcours :

- Interface au d√©marrage :

  ![Formulaire vide](images/appBlank.png)

- Exemple de pr√©diction n√©gative avec retour utilisateur :

  ![Pr√©diction n√©gative](images/appNegatif.png)  
  ![Feedback envoy√©](images/appFeedback.png)

- Exemple de pr√©diction positive :

  ![Pr√©diction positive](images/appPositif.png)

#### Lancement local

Le front est accessible uniquement localement.  
Pour le d√©marrer :

```bash
streamlit run app.py
```

Cela ouvre automatiquement l‚Äôapplication dans le navigateur √† l‚Äôadresse suivante :

```
http://localhost:8501
```

Il est √©galement possible de personnaliser les endpoints dans le code pour pointer vers une instance distante, mais ce front a √©t√© con√ßu principalement √† des fins de test.

**R√©sum√© :**

- D√©velopp√© avec Streamlit
- Permet de tester `/predict` et `/feedback`
- Lancement simple en local
- Non d√©ploy√© dans le cloud

---

### Monitoring et alertes

Une fois l‚ÄôAPI d√©ploy√©e dans Azure Container Apps, il est essentiel d‚Äôassurer un suivi en production pour d√©tecter les erreurs, suivre l‚Äôusage, et alimenter une boucle de retour pour am√©liorer le mod√®le.

Le projet utilise **Azure Application Insights**, int√©gr√© via la biblioth√®que `azure-monitor-opentelemetry` dans le backend FastAPI.

#### Logs et suivi applicatif

√Ä chaque appel √† l‚ÄôAPI, des logs personnalis√©s sont envoy√©s √† Application Insights, notamment lors de la r√©ception de feedback utilisateur.  
Ces logs incluent les champs essentiels : texte, pr√©diction retourn√©e, retour utilisateur, erreurs √©ventuelles, etc.

Voici un aper√ßu de l‚Äôenvironnement Application Insights :

![Application Insights](images/AppInsight.png)

Des tableaux de bord peuvent √™tre cr√©√©s pour visualiser l'activit√© des endpoints, les fr√©quences d‚Äôappels et les erreurs :

![Dashboard personnalis√© Azure Workbook](images/workbook1.png)  
![Autre vue de suivi](images/workbook2.png)  
![Suivi d√©taill√© d‚Äôun feedback](images/workbook3.png)  
![Exploration des donn√©es via workbook](images/workbook4.png)

#### Mise en place des alertes

Des **r√®gles d‚Äôalerte** sont configur√©es √† partir des logs collect√©s. Cela permet d‚Äô√™tre notifi√© si une anomalie est d√©tect√©e.

Exemples de r√®gles mises en place :

- Erreurs fr√©quentes sur l‚ÄôAPI
- Trop grand nombre de feedbacks n√©gatifs sur une p√©riode donn√©e

Exemples de notifications envoy√©es par mail :

![Email d'alerte - 1](images/emailAlert.png)  
![Email d'alerte - 2](images/emailAlert2.png)  
![Email d'alerte - 3](images/emailAlert3.png)

Exemples de r√®gles dans Azure :

![Configuration alerte 1](images/warning1.png)  
![Configuration alerte 2](images/warning2.png)

#### Cas d‚Äôusage : boucle de feedback

Chaque retour utilisateur est consign√© et analysable. Ces donn√©es permettent :

- D‚Äôidentifier des pr√©dictions incorrectes
- D‚Äôalimenter un futur jeu de donn√©es pour l‚Äôam√©lioration du mod√®le
- De d√©clencher des retrainings cibl√©s

Cela alimente une **d√©marche de model monitoring** et d'am√©lioration continue.

**R√©sum√© :**

- Logs envoy√©s automatiquement vers Azure Application Insights
- Tableaux de bord de suivi en temps r√©el (Workbook)
- Alertes configur√©es en cas de d√©rives
- Notifications email op√©rationnelles
- Donn√©es pr√™tes pour l‚Äôanalyse et l‚Äôam√©lioration du mod√®le

---

### Proposition de d√©marche pour l'am√©lioration continue du mod√®le

Bien que le projet n‚Äôint√®gre pas encore de pipeline automatis√© de r√©entra√Ænement, une **d√©marche d‚Äôam√©lioration continue** pourrait √™tre mise en place √† partir des √©l√©ments d√©j√† existants (API en production, retours utilisateurs, logs centralis√©s).

Voici les grandes √©tapes d‚Äôune telle strat√©gie :

#### 1. Collecte automatis√©e des feedbacks

L‚ÄôAPI `/feedback` permet d√©j√† de r√©cup√©rer l‚Äô√©valuation de la pr√©diction par l‚Äôutilisateur (feedback correct ou non).  
Ces donn√©es peuvent √™tre stock√©es dans une base d√©di√©e ou extraites r√©guli√®rement depuis Application Insights.

#### 2. Cr√©ation d‚Äôun jeu de donn√©es compl√©mentaire

Les retours utilisateurs o√π la pr√©diction √©tait incorrecte (feedback n√©gatif) peuvent √™tre utilis√©s pour enrichir le jeu d‚Äôentra√Ænement.  
Cette base peut √™tre filtr√©e, nettoy√©e, puis int√©gr√©e √† une version 2 du dataset.

#### 3. R√©entra√Ænement p√©riodique

Un pipeline de r√©entra√Ænement pourrait √™tre ex√©cut√© de mani√®re :

- Manuelle (ex. via script Python d√©clench√© chaque mois)
- Ou automatis√©e (ex. via GitHub Actions ou Azure Machine Learning)

L‚Äôid√©e serait de recharger le mod√®le avec :

- Le dataset initial
- - les retours utilisateurs corrig√©s
- - √©ventuellement des pond√©rations pour les cas difficiles

#### 4. R√©√©valuation et tra√ßabilit√©

Chaque nouveau mod√®le peut √™tre :

- Compar√© √† l'ancien via MLflow
- Versionn√© avec un identifiant clair
- Test√© automatiquement avant mise en production

#### 5. D√©ploiement contr√¥l√©

Une fois valid√©, le mod√®le mis √† jour peut √™tre d√©ploy√© via Docker + CI/CD, comme dans la version actuelle.  
Un tag (`v2`, `v3`, etc.) permettrait de suivre les √©volutions dans le temps.

#### 6. Boucle d‚Äôapprentissage continue

Ce m√©canisme permettrait d‚Äôimpl√©menter une **boucle vertueuse**, o√π :

- L‚Äôusage r√©el de l‚ÄôAPI alimente le dataset
- Le mod√®le est continuellement ajust√©
- La performance en production s‚Äôam√©liore au fil du temps

Cette strat√©gie constitue une base solide pour aller vers une vraie d√©marche de **‚Äúcontinuous learning‚Äù**, souvent recherch√©e dans les contextes industriels ou √† forte √©volution s√©mantique.

---

## Conclusion

Ce projet de classification de sentiments s‚Äôest appuy√© sur une approche structur√©e m√™lant data science, d√©veloppement API, et bonnes pratiques de MLOps.

Trois mod√®les ont √©t√© test√©s, compar√©s et √©valu√©s afin d‚Äôidentifier la solution la plus performante et la plus l√©g√®re √† d√©ployer (un mod√®le DistilBERT export√© en TensorFlow Lite).  
Un soin particulier a √©t√© apport√© √† la **tra√ßabilit√© des exp√©rimentations**, au **versionnement**, √† la **mise en production via Docker et Azure**, ainsi qu‚Äô√† la **mise en place de tests automatis√©s** pour garantir la qualit√© du code.

L‚Äô**int√©gration d‚ÄôAzure Application Insights** a permis un suivi fin des appels √† l‚ÄôAPI, avec des alertes configurables, offrant ainsi les bases d‚Äôune v√©ritable observabilit√© en production.

Enfin, une **d√©marche projet√©e d‚Äôam√©lioration continue** a √©t√© esquiss√©e, afin d‚Äôexploiter les feedbacks utilisateurs collect√©s et d‚Äôenvisager des mises √† jour futures du mod√®le de mani√®re contr√¥l√©e.

Ce travail montre qu‚Äôil est possible, m√™me avec des ressources limit√©es, de mettre en place une cha√Æne de traitement de machine learning compl√®te, fiable, et pr√™te pour des √©volutions futures en production.
